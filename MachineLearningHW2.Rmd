---
title: "Machine Learning HW 2"
output: html_document
date: "2024-11-11"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggplot2)
library(ggpubr)
library(stats)
library(UsingR)
library(AmesHousing)
library(caret)
library(glmnet)
library(car)
library(earth)
library(DescTools)
library(corrplot)
library(mgcv)
library(visreg)
library(Hmisc)
library(ROCit)
library(MASS)
library(brant)
library(VGAM)
library(randomForest)
library(xgboost)
library(Ckmeans.1d.dp)
library(pdp)
```

```{r}
train <- read_csv('train.csv')
validation <- read_csv('validation.csv')
```



```{r}
# Random Forest

## All variables

tuneGrid <- expand.grid(
  mtry = 4:10
)

set.seed(123)
rf.caret <- caret::train(
INS ~ .,
data = train,
method = "rf",
tuneGrid = tuneGrid,
trControl = trainControl(method = "cv",number = 10,allowParallel = TRUE),
ntree = 500
)

print(rf.caret$bestTune) ## mtry of 8
```

```{r}
set.seed(123)
rf = randomForest(INS ~ ., data = train,ntree = 500,mtry = 8)

pred <- predict(rf,type = "prob")

roc <- rocit(pred[,2],train$INS)

print(roc$AUC)

plot(roc)
```

## 20 Most important variables

```{r}
# Importance
imp <- importance(rf,type = 2)

imp <- imp %>% as.data.frame() %>% arrange(-MeanDecreaseGini)

# 20 most important variables
train2 <- train %>% select(rownames(imp)[1:20],INS)


write.csv(imp,"imp.csv")
```

```{r,eval = F}
# Re tuning mtry with 20 most important

set.seed(123)

rf.caret2 <- caret::train(
INS ~ .,
data = train2,
method = "rf",
tuneGrid = tuneGrid,
trControl = trainControl(method = "cv",number = 10,allowParallel = TRUE),
ntree = 500
)

rf.caret2$bestTune ## Mtry of 5
```

```{r}
rf2 = randomForest(INS ~ ., data = train2,ntree = 500,mtry = 5)

pred2 <- predict(rf2,type = "prob")

roc2 <- rocit(pred2[,2],train$INS)

print(roc2$AUC) 
```

## Random Variable Method

```{r}
set.seed(123)
# Add random noise
trainRand = train %>% mutate(random = rnorm(nrow(train)))

# Make RF with random
set.seed(123)
rfRand = randomForest(INS ~ ., data = trainRand,ntree = 500)

# Importance with random
impRand = importance(rfRand,type = 2) %>% as.data.frame() %>% 
  arrange(-MeanDecreaseGini)

print(impRand)
```
Selecting top 3 + AcctAge bc its almost equal to random

```{r}
# Selecting only important variables
trainImp <- train %>% select(rownames(impRand)[1:3],INS)

set.seed(123)
rfImp <- randomForest(INS ~ ., data = trainImp,ntree = 500)

predImp <- predict(rfImp,type = "prob")

rocImp <- rocit(predImp[,2],train$INS)

print(rocImp$AUC) 
```




```{r}
categorical_vars <- c("DDA", "DIRDEP", "NSF", "SAV", "ATM", "CD", "IRA", "INV", "MM", "CC", "SDB", "INAREA", "BRANCH", "CCPURC", "MMCRED", "INS")

train <- train %>% 
  mutate(across(all_of(categorical_vars), ~ as.factor(.x)))

validation <- validation %>% 
  mutate(across(all_of(categorical_vars), ~ as.factor(.x)))
```


Ava's Random Forest:
```{r}
set.seed(12345)
rf <- randomForest(INS ~ ., data = train, ntree = 500, importance = TRUE)

plot(rf, main = "Number of Trees Compared to MSE")
```


```{r}
# Define the tuning grid
tuneGrid <- expand.grid(mtry = c(2, 4, 6, 8))

# Set up cross-validation
control <- trainControl(method = "cv", number = 5)

# Tune the model
set.seed(42)
rf_tuned <- train(
  INS ~ .,
  data = train,
  method = "rf",
  tuneGrid = tuneGrid,
  trControl = control,
  ntree = 175
)

# View the results
print(rf_tuned)

```

```{r}
rf <- randomForest(INS ~ ., data = train, ntree = 175, mtry = 6, importance = TRUE)

varImpPlot(rf,
           sort = TRUE,
           n.var = 14,
           main = "Order of Variables")

importance(rf, type = 1)
```

```{r}
train$random <- rnorm(8495) #new random variable every time you run

set.seed(42)
rf2 <- randomForest(INS ~ ., data = train, ntree = 175, mtry = 6, importance = TRUE)

par(mar = c(10, 4, 4, 2))
varImpPlot(rf2,
           sort = TRUE,
           n.var = 50)
```




```{r}
train$p_hat <- predict(rf, type = "prob")[, 2]

logit_roc <- rocit(as.numeric(train$p_hat), as.numeric(train$INS))
plot(logit_roc)$optimal
summary(logit_roc)
```


XG Boost:
```{r}
train <- read_csv('train.csv')
validation <- read_csv('validation.csv')
categorical_vars <- c("DDA", "DIRDEP", "NSF", "SAV", "ATM", "CD", "IRA", "INV", "MM", "CC", "SDB", "INAREA", "BRANCH", "CCPURC", "MMCRED", "INS")

train <- train %>% 
  mutate(across(all_of(categorical_vars), ~ as.factor(.x)))

validation <- validation %>% 
  mutate(across(all_of(categorical_vars), ~ as.factor(.x)))

train_x <- model.matrix(INS~ ., data = train)[, -1]
train_y <- as.numeric(as.character(train$INS))

set.seed(12345)
xgb.ames <- xgboost(data = train_x, label = train_y, subsample = 0.5, nrounds = 50, params = list(objective = "binary:logistic", eval_metric = "auc"))
```
```{r}
xgbcv <- xgb.cv(data = train_x, label = train_y, subsample = 0.5, nrounds = 100, params = list(objective = "binary:logistic", eval_metric = "auc"), nfold = 10)
```

```{r}
tune_grid <- expand.grid(
  nrounds = 20,
  eta = c(0.1, 0.15, 0.2, 0.25, 0.3),
  max_depth = c(1:10),
  gamma = c(0),
  colsample_bytree = 1,
  min_child_weight = 1,
  subsample = c(0.25, 0.5, 0.75, 1)
)

set.seed(12345)
xgb.caret <- train(x = train_x, y = as.factor(train_y),
      method = "xgbTree",
      tuneGrid = tune_grid,
      trControl = trainControl(method = 'cv', # Using 10-fold cross-validation
                               number = 10))

plot(xgb.caret)
```

```{r}
xgb.caret$bestTune
```

```{r}
xgb <- xgboost(data = train_x, label = train_y,  subsample = 0.75, nrounds = 20, eta = 0.2, max_depth = 4, params = list(objective = "binary:logistic", eval_metric = "auc"))

xgb.importance(feature_names = colnames(train_x), model = xgb)

xgb.ggplot.importance(xgb.importance(feature_names = colnames(train_x), model = xgb))
```




```{r}
train$random <- rnorm(8495)

train_x <- model.matrix(INS~ ., data = train)[, -1]
train_y <- as.numeric(as.character(train$INS))

set.seed(12345)
xgb <- xgboost(data = train_x, label = train_y,  subsample = 0.75, nrounds = 20, eta = 0.2, max_depth = 4, params = list(objective = "binary:logistic", eval_metric = "auc"))

xgb.importance(feature_names = colnames(train_x), model = xgb)

xgb.ggplot.importance(xgb.importance(feature_names = colnames(train_x), model = xgb))
```

```{r}
cv <- xgb.cv(
  data = train_x,
  label = train_y,
   subsample = 0.75, nrounds = 20, eta = 0.2, max_depth = 4,
  nfold = 10,  # Number of folds for cross-validation
  metrics = "auc",
  objective = "binary:logistic"
)

mean_auc <- cv$evaluation_log$test_auc_mean[27]  # 27 is the number of rounds
print(mean_auc)
```


```{r}
train_x2 <- train_x[, c('DDA1', 'SAVBAL', 'DDABAL', 'CDBAL', 'MMBAL', 'ACCTAGE', 'DEPAMT', 'CHECKS', 'ATMAMT', 'IRABAL', 'BRANCHB15', 'TELLER', 'CCBAL', 'CRSCORE', 'BRANCHB14', 'CC1')]
train_y2 <- as.numeric(as.character(train$INS))

set.seed(12345)
xgb2 <- xgboost(data = train_x2, label = train_y2,  subsample = 0.75, nrounds = 20, eta = 0.2, max_depth = 4, params = list(objective = "binary:logistic", eval_metric = "auc"))

train_x2$p_hat <- predict(xgb2, train_x2, type = "prob")

logit_roc2 <- rocit(as.numeric(train_x2$p_hat), as.numeric(train_y2))
plot(logit_roc2)$optimal
summary(logit_roc2)
```


```{r}
train_x$p_hat <- predict(xgb, train_x, type = "prob")

logit_roc <- rocit(as.numeric(train_x$p_hat), as.numeric(train_y))
plot(logit_roc)$optimal
summary(logit_roc)

```

```{r}
threshold <- 0.3646002 
pred_class <- ifelse(train_x$p_hat  > threshold, 1, 0)

# Step 3: Generate confusion matrix
confusion_matrix <- table(Predicted = pred_class, Actual = train_y)
print(confusion_matrix)
```

```{r}
#TPR
2184/(2184+734)


#FPR
1299/(1299+4278)

#Accuracy
(2184+4278)/(2184+734+1299+4278)
```


```{r}
valid_x <- model.matrix(INS~ ., data = validation)[, -1]
valid_y <- as.numeric(as.character(validation$INS))

valid_x$p_hat <- predict(xgb, valid_x, type = "prob")

logit_roc <- rocit(as.numeric(valid_x$p_hat), as.numeric(valid_y))
plot(logit_roc)$optimal
summary(logit_roc)

```


```{r}
colnames(valid_x)
```

```{r}
colnames(train_x)
```

```{r}
library(nnet)
library(reshape2)
library(e1071)
library(klaR)
library(iml)
library(patchwork)

pred <- predict(xgb,type = "prob")
ale_plot <- FeatureEffects$new(pred, method = "ale")
```




